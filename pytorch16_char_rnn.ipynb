{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 古诗词对子生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 训练数据查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.315656Z",
     "start_time": "2018-02-18T03:28:52.286844Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用古诗来作为例子，读取数据，看看长什么样子\n",
    "with open('./char_dataset/poetry.txt', 'r') as f:\n",
    "    poetry_corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.331908Z",
     "start_time": "2018-02-18T03:28:52.317790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变，春逐鸟声开\n"
     ]
    }
   ],
   "source": [
    "print poetry_corpus[:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.353185Z",
     "start_time": "2018-02-18T03:28:52.340405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变 春逐鸟声开 \n"
     ]
    }
   ],
   "source": [
    "# 为了可视化比较方便，我们将一些其他字符替换成空格\n",
    "poetry_corpus = poetry_corpus.replace('\\n', '').replace('\\r', '').replace('，', ' ').replace('。', ' ')\n",
    "print poetry_corpus.decode('utf-8')[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry_corpus = poetry_corpus.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.338277Z",
     "start_time": "2018-02-18T03:28:52.334069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看看字符数\n",
    "len(poetry_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 训练数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本数值表示, 将文字转换成数字，对所有非重复的字符，可以从 0 开始建立索引\n",
    "# 同时为了节省内存的开销，可以词频比较低的字去掉\n",
    "import numpy as np\n",
    "\n",
    "class TextConverter(object):\n",
    "    def __init__(self, text_path, max_vocab=5000):\n",
    "        \"\"\"\n",
    "        建立一个字符索引转换器\n",
    "        Args:\n",
    "            text_path: 文本位置\n",
    "            max_vocab: 最大的单词数量\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(text_path, 'r') as f:\n",
    "            tt = f.read()\n",
    "        tt = tt.replace('\\n', '').replace('\\r', '').replace('，', ' ').replace('。', ' ')\n",
    "        text = tt.decode('utf-8')\n",
    "        \n",
    "        # 去掉重复的字符\n",
    "        vocab = set(text)\n",
    "\n",
    "        # 如果单词总数超过最大数值，去掉频率最低的\n",
    "        vocab_count = {}\n",
    "        \n",
    "        # 计算单词出现频率并排序\n",
    "        for word in vocab:\n",
    "            vocab_count[word] = 0\n",
    "        for word in text:\n",
    "            vocab_count[word] += 1\n",
    "        vocab_count_list = []\n",
    "        for word in vocab_count:\n",
    "            vocab_count_list.append((word, vocab_count[word]))\n",
    "        vocab_count_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 如果超过最大值，截取频率最低的字符\n",
    "        if len(vocab_count_list) > max_vocab:\n",
    "            vocab_count_list = vocab_count_list[:max_vocab]\n",
    "        vocab = [x[0] for x in vocab_count_list]\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.word_to_int_table = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.int_to_word_table = dict(enumerate(self.vocab))\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab) + 1\n",
    "\n",
    "    def word_to_int(self, word):\n",
    "        if word in self.word_to_int_table:\n",
    "            return self.word_to_int_table[word]\n",
    "        else:\n",
    "            return len(self.vocab)\n",
    "\n",
    "    def int_to_word(self, index):\n",
    "        if index == len(self.vocab):\n",
    "            return '<unk>'\n",
    "        elif index < len(self.vocab):\n",
    "            return self.int_to_word_table[index]\n",
    "        else:\n",
    "            raise Exception('Unknown index!')\n",
    "\n",
    "    def text_to_arr(self, text):\n",
    "        arr = []\n",
    "        for word in text:\n",
    "            arr.append(self.word_to_int(word))\n",
    "        return np.array(arr)\n",
    "\n",
    "    def arr_to_text(self, arr):\n",
    "        words = []\n",
    "        for index in arr:\n",
    "            words.append(self.int_to_word(index))\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.016322Z",
     "start_time": "2018-02-18T03:28:52.645616Z"
    }
   },
   "outputs": [],
   "source": [
    "convert = TextConverter('./char_dataset/poetry.txt', max_vocab=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.025196Z",
     "start_time": "2018-02-18T03:28:53.018514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变 春逐鸟声开 \n",
      "[ 74 155 415 739 380   0   6 189 100  79  31   0]\n"
     ]
    }
   ],
   "source": [
    "# 可以可视化一下数字表示的字符\n",
    "# 原始文本字符\n",
    "txt_char = poetry_corpus[:12]\n",
    "print(txt_char)\n",
    "\n",
    "# 转化成数字\n",
    "print(convert.text_to_arr(txt_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 构造训练样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.036447Z",
     "start_time": "2018-02-18T03:28:53.027222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "# 构造时序样本数据\n",
    "n_step = 60\n",
    "\n",
    "# 总的序列个数\n",
    "num_seq = int(len(poetry_corpus) / n_step)\n",
    "\n",
    "# 去掉最后不足一个序列长度的部分\n",
    "text = poetry_corpus[:num_seq*n_step]\n",
    "\n",
    "print(num_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.258155Z",
     "start_time": "2018-02-18T03:28:53.038479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[235 108   0 104 120 263   4 299   0 248  68  90 208 292   0]\n",
      "torch.Size([1200, 60])\n",
      "\n",
      "    3\n",
      " 2598\n",
      "  206\n",
      "    4\n",
      "   58\n",
      "    0\n",
      "    1\n",
      "  145\n",
      "   37\n",
      "   68\n",
      "  172\n",
      "    0\n",
      "  149\n",
      "  518\n",
      " 1194\n",
      "    2\n",
      "  119\n",
      "    0\n",
      "  251\n",
      "  100\n",
      "   79\n",
      "   39\n",
      "  670\n",
      "    0\n",
      "  394\n",
      "  156\n",
      " 2981\n",
      "  300\n",
      "  784\n",
      "    0\n",
      "   47\n",
      " 1444\n",
      "  275\n",
      "  177\n",
      " 1020\n",
      "    0\n",
      "   30\n",
      "   34\n",
      "    6\n",
      "  541\n",
      "  792\n",
      "    0\n",
      "  464\n",
      "   42\n",
      "  100\n",
      "    4\n",
      " 1373\n",
      "    0\n",
      "  247\n",
      "   97\n",
      "  127\n",
      "  757\n",
      "   35\n",
      "    0\n",
      "  217\n",
      "  153\n",
      "   67\n",
      "  171\n",
      "   71\n",
      "    0\n",
      "[torch.LongTensor of size 60]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 将序列中所有的文字转换成数字表示，重新排列成 (num_seq x n_step) 的矩阵\n",
    "arr = convert.text_to_arr(text)\n",
    "print(arr[15:30])\n",
    "\n",
    "arr = arr.reshape((num_seq, -1))\n",
    "arr = torch.from_numpy(arr)\n",
    "\n",
    "print(arr.shape)\n",
    "print(arr[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.945768Z",
     "start_time": "2018-02-18T03:28:53.925488Z"
    }
   },
   "outputs": [],
   "source": [
    "# 据此，我们可以构建 PyTorch 中的数据读取来训练网络，\n",
    "# 这里我们将最后一个字符的输出 label 定为输入的第一个字符，也就是\"床前明月光\"的输出是\"前明月光床\"\n",
    "class TextDataset(object):\n",
    "    def __init__(self, arr):\n",
    "        self.arr = arr\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        x = self.arr[item, :]\n",
    "        \n",
    "        # 构造 label\n",
    "        y = torch.zeros(x.shape)\n",
    "        # 将输入的第一个字符作为最后一个输入的 label\n",
    "        y[:-1], y[-1] = x[1:], x[0]\n",
    "        #y[:-3],y[-3:] = x[3:],x[:3]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.arr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.950296Z",
     "start_time": "2018-02-18T03:28:53.947697Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = TextDataset(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以取出其中一个数据集参看一下是否是我们描述的这样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.957705Z",
     "start_time": "2018-02-18T03:28:53.952232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日晃百花色 风动千林翠 池鱼跃不同 园鸟声还异 寄言博通者 知予物外志 一朝春夏改 隔夜鸟花迁 阴阳深浅叶 晓夕重轻烟 \n",
      "晃百花色 风动千林翠 池鱼跃不同 园鸟声还异 寄言博通者 知予物外志 一朝春夏改 隔夜鸟花迁 阴阳深浅叶 晓夕重轻烟 日\n"
     ]
    }
   ],
   "source": [
    "# 可以取出其中一个数据,看一下是否是描述的这样\n",
    "x, y = train_set[1]\n",
    "print(convert.arr_to_text(x.numpy()))\n",
    "print(convert.arr_to_text(y.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 构造模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:54.022455Z",
     "start_time": "2018-02-18T03:28:53.959687Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_gpu = False\n",
    "\n",
    "# 模型可以定义成非常简单的三层，第一层是词嵌入，第二层是 RNN 层，\n",
    "# 因为最后是一个分类问题，所以第三层是线性层，最后输出预测的字符\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim, hidden_size, \n",
    "                 num_layers, dropout):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.word_to_vec = nn.Embedding(num_classes, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_size, num_layers, dropout)\n",
    "        #self.rnn = nn.LSTM(embed_dim, hidden_size, num_layers, dropout)\n",
    "        self.project = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, hs=None):\n",
    "        batch = x.shape[0]\n",
    "        if hs is None:\n",
    "            hs = Variable(\n",
    "                torch.zeros(self.num_layers, batch, self.hidden_size))\n",
    "            if use_gpu:\n",
    "                hs = hs.cuda()\n",
    "        word_embed = self.word_to_vec(x)  # (batch, len, embed)\n",
    "        word_embed = word_embed.permute(1, 0, 2)  # (len, batch, embed)\n",
    "        out, h0 = self.rnn(word_embed, hs)  # (len, batch, hidden)\n",
    "        le, mb, hd = out.shape\n",
    "        out = out.view(le * mb, hd)\n",
    "        out = self.project(out)\n",
    "        out = out.view(le, mb, -1)\n",
    "        out = out.permute(1, 0, 2).contiguous()  # (batch, len, hidden)\n",
    "        return out.view(-1, out.shape[2]), h0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练模型的时候，我们知道这是一个分类问题，所以可以使用交叉熵作为 loss 函数，在语言模型中，我们通常使用一个新的指标来评估结果，这个指标叫做困惑度(perplexity)，可以简单地看作对交叉熵取指数，这样其范围就是 $[1, +\\infty]$，也是越小越好。\n",
    "\n",
    "另外，我们前面讲过 RNN 存在着梯度爆炸的问题，所以我们需要进行梯度裁剪，在 pytorch 中使用 `torch.nn.utils.clip_grad_norm` 就能够轻松实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:54.030508Z",
     "start_time": "2018-02-18T03:28:54.024511Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "train_data = DataLoader(train_set, batch_size, True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:59.955521Z",
     "start_time": "2018-02-18T03:28:54.032512Z"
    }
   },
   "outputs": [],
   "source": [
    "from util.trainer import ScheduledOptim\n",
    "\n",
    "model = CharRNN(convert.vocab_size, 512, 512, 2, 0.5)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "basic_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = ScheduledOptim(basic_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "epoch: 1, perplexity is: 1208.366, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 2, perplexity is: 587.341, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 3, perplexity is: 475.234, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 4, perplexity is: 406.376, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 5, perplexity is: 372.025, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 6, perplexity is: 348.377, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 7, perplexity is: 326.711, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 8, perplexity is: 307.925, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 9, perplexity is: 288.974, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 10, perplexity is: 264.509, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 11, perplexity is: 243.038, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 12, perplexity is: 218.856, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 13, perplexity is: 195.536, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 14, perplexity is: 173.147, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 15, perplexity is: 150.774, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 16, perplexity is: 130.804, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 17, perplexity is: 112.872, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 18, perplexity is: 96.170, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 19, perplexity is: 81.625, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 20, perplexity is: 68.179, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 21, perplexity is: 56.596, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 22, perplexity is: 47.829, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 23, perplexity is: 39.366, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 24, perplexity is: 32.086, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 25, perplexity is: 25.937, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 26, perplexity is: 21.086, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 27, perplexity is: 17.278, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 28, perplexity is: 14.267, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 29, perplexity is: 11.637, lr:1.0e-03\n",
      "5\n",
      "10\n",
      "epoch: 30, perplexity is: 9.398, lr:1.0e-03\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for e in range(epochs):\n",
    "    step_nu = 0\n",
    "    train_loss = 0\n",
    "    for data in train_data:\n",
    "        step_nu += 1\n",
    "        if (step_nu%5 == 0):\n",
    "            print step_nu\n",
    "        x, y = data\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        x, y = Variable(x), Variable(y)\n",
    "\n",
    "        # Forward.\n",
    "        score, _ = model(x)\n",
    "        loss = criterion(score, y.view(-1))\n",
    "\n",
    "        # Backward.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradient.\n",
    "        nn.utils.clip_grad_norm(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "    print('epoch: {}, perplexity is: {:.3f}, lr:{:.1e}'.format(e+1, np.exp(train_loss / len(train_data)), optimizer.lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成文本的过程非常简单，前面已将讲过了，给定了开始的字符，然后不断向后生成字符，将生成的字符作为新的输入再传入网络。\n",
    "# 这里需要注意的是，为了增加更多的随机性，我们会在预测的概率最高的前五个里面依据他们的概率来进行随机选择。\n",
    "\n",
    "def pick_top_n(preds, top_n=5):\n",
    "    top_pred_prob, top_pred_label = torch.topk(preds, top_n, 1)\n",
    "    top_pred_prob /= torch.sum(top_pred_prob)\n",
    "    top_pred_prob = top_pred_prob.squeeze(0).cpu().numpy()\n",
    "    top_pred_label = top_pred_label.squeeze(0).cpu().numpy()\n",
    "    c = np.random.choice(top_pred_label, size=1, p=top_pred_prob)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   71   144   341  3223    13\n",
      "[torch.LongTensor of size 1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "begin = '烟雨飘漫天'\n",
    "#begin = '随风潜入夜'\n",
    "begin = begin.decode('utf-8')\n",
    "text_len = 36\n",
    "\n",
    "model = model.eval()\n",
    "samples = [convert.word_to_int(c) for c in begin]\n",
    "input_txt = torch.LongTensor(samples)[None]\n",
    "print input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 13\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if use_gpu:\n",
    "    input_txt = input_txt.cuda()\n",
    "input_txt = Variable(input_txt)\n",
    "_, init_state = model(input_txt)\n",
    "result = samples\n",
    "model_input = input_txt[:, -1][:, None]\n",
    "print model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(text_len):\n",
    "    out, init_state = model(model_input, init_state)\n",
    "    pred = pick_top_n(out.data)\n",
    "    model_input = Variable(torch.LongTensor(pred))[None]\n",
    "    if use_gpu:\n",
    "        model_input = model_input.cuda()\n",
    "    result.append(pred[0])\n",
    "    #result.append(out.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 144, 341, 3223, 13, 0, 9, 154, 275, 135, 20, 578, 0, 66, 149, 718, 33, 543, 0, 239, 446, 892, 75, 441, 1072, 0, 60, 155, 154, 134, 154, 1148, 161, 1148, 103, 103, 58, 145, 0, 1, 4]\n",
      "烟雨飘漫天 山文物华空侧 芳池写清溪 岩峰势出轮质 影随文凤文短野短草草色动 风花\n"
     ]
    }
   ],
   "source": [
    "print result\n",
    "text = convert.arr_to_text(result)\n",
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
